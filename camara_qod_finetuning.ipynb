{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMARA QoD API Fine-tuning with QLoRA\n",
    "\n",
    "This notebook demonstrates fine-tuning a small language model (Phi-3-Mini) to become an expert assistant for the CAMARA Quality on Demand API.\n",
    "\n",
    "**Hardware Requirements:** T4 GPU (Google Colab Free Tier compatible)\n",
    "\n",
    "**Training Method:** QLoRA with Unsloth for 2x faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -U psutil unsloth transformers datasets trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model with QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True  # Enable 4-bit quantization\n",
    "\n",
    "# Load model with Unsloth optimization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PEFT (Parameter-Efficient Fine-Tuning) with LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimization\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Format Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruction prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples into instruction-input-response format\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"Prompt template defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSONL file\n",
    "# Upload sft_dataset.jsonl to Colab first\n",
    "dataset = load_dataset(\"json\", data_files=\"sft_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "print(\"\\nSample formatted example:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Base Model (Before Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"\"\"I'm at a crowded stadium and need better upload for a 4K stream. My phone number is +14155551234 and I'm streaming to server 198.51.100.50 for the next 2 hours.\"\"\"\n",
    "\n",
    "# Format test input\n",
    "test_input = alpaca_prompt.format(\n",
    "    \"You are an expert assistant for the CAMARA Quality on Demand (QoD) API. Convert user requests into valid API calls.\",\n",
    "    test_query,\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Generate response from base model\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([test_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"=== BASE MODEL OUTPUT (Before Fine-tuning) ===\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3)\n",
    "response_before = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response_before.split(\"### Response:\")[-1].strip())\n",
    "\n",
    "# Save for comparison\n",
    "response_before_saved = response_before.split(\"### Response:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camara_qod_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=False,  # Can enable for efficiency if needed\n",
    ")\n",
    "\n",
    "print(\"SFT Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the same query\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([test_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"=== FINE-TUNED MODEL OUTPUT (After Training) ===\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3)\n",
    "response_after = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response_after.split(\"### Response:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Validate JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and validate JSON\n",
    "try:\n",
    "    response_text = response_after.split(\"### Response:\")[-1].strip()\n",
    "    json_obj = json.loads(response_text)\n",
    "    print(\"\u2705 Valid JSON output!\")\n",
    "    print(\"\\nParsed structure:\")\n",
    "    print(json.dumps(json_obj, indent=2))\n",
    "    \n",
    "    # Check for required CAMARA fields\n",
    "    required_fields = [\"device\", \"applicationServer\", \"qosProfile\", \"duration\"]\n",
    "    missing_fields = [f for f in required_fields if f not in json_obj]\n",
    "    \n",
    "    if missing_fields:\n",
    "        print(f\"\\n\u26a0\ufe0f Missing required fields: {missing_fields}\")\n",
    "    else:\n",
    "        print(\"\\n\u2705 All required CAMARA fields present!\")\n",
    "        \nexcept json.JSONDecodeError as e:\n",
    "    print(f\"\u274c Invalid JSON: {e}\")\n",
    "    print(\"Response:\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various scenarios\n",
    "test_cases = [\n",
    "    \"Need ultra-low latency for VR gaming. Device IP 203.0.113.75, server 192.0.2.200, 3 hours.\",\n",
    "    \"Video conference with IPv6 2001:db8::1 to server 2001:db8:1234::1 for 45 minutes.\",\n",
    "    \"IoT sensor data upload from phone +12025551111 to cloud 10.0.0.100, 15 minutes.\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Case {i}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    test_prompt = alpaca_prompt.format(\n",
    "        \"You are an expert assistant for the CAMARA Quality on Demand (QoD) API. Convert user requests into valid API calls.\",\n",
    "        query,\n",
    "        \"\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    result = response.split(\"### Response:\")[-1].strip()\n",
    "    print(result)\n",
    "    \n",
    "    # Validate JSON\n",
    "    try:\n",
    "        json.loads(result)\n",
    "        print(\"\\n\u2705 Valid JSON\")\n",
    "    except:\n",
    "        print(\"\\n\u274c Invalid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"camara_qod_lora_model\")\n",
    "tokenizer.save_pretrained(\"camara_qod_lora_model\")\n",
    "\n",
    "print(\"Model saved to: camara_qod_lora_model/\")\n",
    "\n",
    "# Optional: Save to HuggingFace Hub\n",
    "# model.push_to_hub(\"your-username/camara-qod-phi3-lora\")\n",
    "# tokenizer.push_to_hub(\"your-username/camara-qod-phi3-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Merged Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model for standalone deployment\n",
    "model.save_pretrained_merged(\n",
    "    \"camara_qod_merged_model\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(\"Merged model saved to: camara_qod_merged_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. \u2705 Loading Phi-3-Mini with 4-bit quantization\n",
    "2. \u2705 Configuring QLoRA (r=16, alpha=16, dropout=0.05)\n",
    "3. \u2705 Fine-tuning on 50 CAMARA QoD examples\n",
    "4. \u2705 Validating outputs against API specification\n",
    "5. \u2705 Saving model for deployment\n",
    "\n",
    "**Next Steps:**\n",
    "- Run DPO training with preference dataset\n",
    "- Deploy model to inference API\n",
    "- Create performance report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}